# GRPO Ablation Study: REASONING + CONCLUSION only
# No EXPLANATION tag, no explanation reward
#
# Usage:
#   bash external/ms-swift/examples/train/grpo/internal/run_grpo.sh \
#       configs/grpo/ablation_think_answer.yaml

# Model Configuration
model:
  id_or_path: "OpenGVLab/InternVL3_5-2B"
  type: "internvl3"
  
# Data Configuration (use think_answer mode data)
data:
  train_dataset: "/mnt/VLAI_data/ViVQA-X/processed/ViVQA-X_train_think_answer.jsonl"
  max_length: 4096
  max_completion_length: 1024
  
# Reward Configuration
rewards:
  plugin_path: "external/ms-swift/examples/train/grpo/plugin/plugin.py"
  functions:
    - vinle_format_think_answer  # Validates REASONING + CONCLUSION tags
    - vinle_accuracy             # ROUGE-L + BERTScore on CONCLUSION
  # NO vinle_explanation - this is the ablation
  
# GRPO Parameters
grpo:
  num_generations: 4
  temperature: 0.9
  top_p: 0.9
  top_k: 50
  beta: 0.04
  log_completions: true
  
# Training Configuration
training:
  train_type: "lora"
  lora_rank: 32
  lora_alpha: 64
  target_modules: "all-linear"
  freeze_vit: true
  
  # Optimization
  num_train_epochs: 2
  max_steps: 2000
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 1.0e-5
  warmup_ratio: 0.05
  
  # Checkpointing
  save_steps: 50
  save_total_limit: 2
  save_only_model: false
  
  # Logging & Evaluation
  logging_steps: 1
  eval_strategy: "steps"
  eval_steps: 1
  report_to: "wandb"
  
  # Performance
  torch_dtype: "bfloat16"
  attn_impl: "flash_attention_2"
  gradient_checkpointing: true
  dataloader_num_workers: 16
  dataset_num_proc: 16
  
  # Quantization
  quant_method: "bnb"
  quant_bits: 4
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  
# Output Configuration
output:
  dir: "output/grpo/ablation_think_answer"
  resume_from_checkpoint: null
  
# Environment
environment:
  cuda_visible_devices: "2"
  pytorch_cuda_alloc_conf: "expandable_segments:True"
  hf_endpoint: "https://huggingface.co"
  use_vllm: false
  use_hf: true
