# SFT Baseline: Supervised Fine-tuning
# CONCLUSION + EXPLANATION format (no REASONING)
#
# Usage:
#   bash external/ms-swift/examples/train/sft/run_sft.sh \
#       configs/sft/baseline.yaml

# Model Configuration
model:
  id_or_path: "OpenGVLab/InternVL3_5-2B"
  type: "internvl3"
  
# Data Configuration (use SFT mode data)
data:
  train_dataset: "/mnt/VLAI_data/ViVQA-X/processed/ViVQA-X_train_sft.jsonl"
  max_length: 4096
  
# Training Configuration
training:
  train_type: "lora"
  lora_rank: 32
  lora_alpha: 64
  target_modules: "all-linear"
  freeze_vit: true
  
  # Optimization
  num_train_epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  learning_rate: 5.0e-5
  warmup_ratio: 0.03
  
  # Checkpointing
  save_steps: 100
  save_total_limit: 3
  save_only_model: false
  
  # Logging & Evaluation
  logging_steps: 10
  eval_strategy: "steps"
  eval_steps: 100
  report_to: "wandb"
  
  # Performance
  torch_dtype: "bfloat16"
  attn_impl: "flash_attention_2"
  gradient_checkpointing: true
  dataloader_num_workers: 8
  dataset_num_proc: 8
  
  # Quantization
  quant_method: "bnb"
  quant_bits: 4
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  
# Output Configuration
output:
  dir: "output/sft/baseline"
  resume_from_checkpoint: null
  
# Environment
environment:
  cuda_visible_devices: "2"
  pytorch_cuda_alloc_conf: "expandable_segments:True"
  hf_endpoint: "https://huggingface.co"
